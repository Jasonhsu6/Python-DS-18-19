{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA and LDA to news group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training news: 11293\n",
      "number of test news: 7528\n"
     ]
    }
   ],
   "source": [
    "with open('20ng-train-all-terms.txt') as f:\n",
    "    train = f.read()\n",
    "with open('20ng-test-all-terms.txt') as f:\n",
    "    test = f.read()\n",
    "\n",
    "train = train[:-1]\n",
    "test = test[:-1]\n",
    "topic_train = []\n",
    "topic_test = []\n",
    "news_train = []\n",
    "news_test = []\n",
    "for new in train.split('\\n'):\n",
    "    topic_train.append(new.split('\\t')[0])\n",
    "    news_train.append(new.split('\\t')[1])\n",
    "print('number of training news: ' + str(len(news_train)))\n",
    "for new in test.split('\\n'):\n",
    "    topic_test.append(new.split('\\t')[0])\n",
    "    news_test.append(new.split('\\t')[1])\n",
    "print('number of test news: ' + str(len(news_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(topic_test)\n",
    "X_test = np.array(news_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of news: 11293\n",
      "number of topics: 7528\n"
     ]
    }
   ],
   "source": [
    "print('number of news: ' + str(len(news_train)))\n",
    "print('number of topics: ' + str(len(topic_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CountVectorizer to find three letter tokens, remove stop_words,\n",
    "# remove tokens that don't appear in at least 20 documents,\n",
    "# remove tokens that appear in more than 20% of the documents\n",
    "vect = CountVectorizer(min_df = 20, max_df = 0.2, stop_words = 'english', token_pattern = '(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b', \n",
    "                       ngram_range = (1, 2))\n",
    "X = vect.fit_transform(news_train)\n",
    "\n",
    "# Convert sparse matrix to gensim corpus\n",
    "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns = False)\n",
    "id_map = dict((v, k) for k, v in vect.vocabulary_.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 20, id2word = id_map, passes = 25)\n",
    "# ldamodel = gensim.models.LdaMulticore(corpus, num_topics = 6, id2word = id_map, passes = 25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"government\" + 0.013*\"president\" + 0.009*\"states\" + 0.009*\"state\" + 0.009*\"american\" + 0.007*\"health\" + 0.007*\"rights\" + 0.007*\"clinton\" + 0.006*\"country\" + 0.006*\"anti\"'),\n",
       " (1,\n",
       "  '0.014*\"state\" + 0.012*\"article apr\" + 0.012*\"pitt\" + 0.011*\"ohio\" + 0.011*\"john\" + 0.010*\"writes article\" + 0.009*\"amendment\" + 0.009*\"ohio state\" + 0.009*\"pitt edu\" + 0.008*\"org\"'),\n",
       " (2,\n",
       "  '0.027*\"gun\" + 0.014*\"law\" + 0.013*\"crime\" + 0.013*\"police\" + 0.012*\"turks\" + 0.011*\"uiuc\" + 0.010*\"uiuc edu\" + 0.010*\"weapons\" + 0.009*\"tax\" + 0.008*\"court\"'),\n",
       " (3,\n",
       "  '0.015*\"arms\" + 0.015*\"gay\" + 0.013*\"men\" + 0.012*\"objective\" + 0.012*\"frank\" + 0.011*\"virginia\" + 0.011*\"writes article\" + 0.011*\"sex\" + 0.010*\"morality\" + 0.009*\"sexual\"'),\n",
       " (4,\n",
       "  '0.023*\"article apr\" + 0.018*\"colorado\" + 0.018*\"isc\" + 0.015*\"henrik\" + 0.015*\"boston\" + 0.014*\"henry\" + 0.014*\"colorado edu\" + 0.014*\"writes article\" + 0.013*\"toronto\" + 0.012*\"rochester\"'),\n",
       " (5,\n",
       "  '0.009*\"believe\" + 0.007*\"evidence\" + 0.007*\"question\" + 0.007*\"true\" + 0.006*\"law\" + 0.005*\"fact\" + 0.005*\"say\" + 0.005*\"argument\" + 0.005*\"truth\" + 0.005*\"right\"'),\n",
       " (6,\n",
       "  '0.022*\"rutgers\" + 0.021*\"rutgers edu\" + 0.017*\"article apr\" + 0.011*\"pts\" + 0.009*\"group\" + 0.009*\"utexas\" + 0.009*\"utexas edu\" + 0.008*\"university\" + 0.007*\"fred\" + 0.007*\"robert\"'),\n",
       " (7,\n",
       "  '0.008*\"make\" + 0.007*\"really\" + 0.007*\"want\" + 0.007*\"better\" + 0.007*\"way\" + 0.006*\"going\" + 0.005*\"work\" + 0.005*\"sure\" + 0.005*\"probably\" + 0.005*\"lot\"'),\n",
       " (8,\n",
       "  '0.025*\"god\" + 0.012*\"jesus\" + 0.009*\"say\" + 0.008*\"christian\" + 0.007*\"said\" + 0.007*\"bible\" + 0.007*\"life\" + 0.007*\"did\" + 0.007*\"says\" + 0.006*\"christians\"'),\n",
       " (9,\n",
       "  '0.016*\"mail\" + 0.013*\"available\" + 0.011*\"file\" + 0.011*\"information\" + 0.011*\"list\" + 0.010*\"ftp\" + 0.009*\"software\" + 0.009*\"send\" + 0.009*\"email\" + 0.008*\"files\"'),\n",
       " (10,\n",
       "  '0.027*\"turkish\" + 0.012*\"security\" + 0.012*\"university\" + 0.011*\"administration\" + 0.011*\"new\" + 0.010*\"secret\" + 0.010*\"information\" + 0.009*\"technology\" + 0.008*\"public\" + 0.008*\"gov\"'),\n",
       " (11,\n",
       "  '0.012*\"said\" + 0.009*\"right\" + 0.009*\"didn\" + 0.008*\"did\" + 0.008*\"league\" + 0.008*\"home\" + 0.008*\"went\" + 0.007*\"city\" + 0.007*\"drug\" + 0.007*\"came\"'),\n",
       " (12,\n",
       "  '0.022*\"israel\" + 0.009*\"war\" + 0.008*\"said\" + 0.007*\"military\" + 0.007*\"did\" + 0.007*\"land\" + 0.007*\"killed\" + 0.006*\"soldiers\" + 0.006*\"soviet\" + 0.006*\"new\"'),\n",
       " (13,\n",
       "  '0.033*\"jews\" + 0.029*\"armenian\" + 0.026*\"armenians\" + 0.016*\"armenia\" + 0.016*\"jewish\" + 0.016*\"turkey\" + 0.013*\"russian\" + 0.011*\"greek\" + 0.010*\"genocide\" + 0.008*\"muslim\"'),\n",
       " (14,\n",
       "  '0.030*\"game\" + 0.030*\"team\" + 0.019*\"games\" + 0.019*\"play\" + 0.017*\"hockey\" + 0.016*\"year\" + 0.016*\"season\" + 0.012*\"win\" + 0.011*\"baseball\" + 0.011*\"new\"'),\n",
       " (15,\n",
       "  '0.012*\"use\" + 0.012*\"file\" + 0.011*\"windows\" + 0.011*\"window\" + 0.010*\"program\" + 0.009*\"using\" + 0.007*\"problem\" + 0.007*\"output\" + 0.007*\"line\" + 0.006*\"color\"'),\n",
       " (16,\n",
       "  '0.024*\"car\" + 0.009*\"dod\" + 0.008*\"cars\" + 0.008*\"gas\" + 0.007*\"new\" + 0.007*\"gov\" + 0.006*\"engine\" + 0.006*\"myers\" + 0.006*\"water\" + 0.006*\"oil\"'),\n",
       " (17,\n",
       "  '0.024*\"space\" + 0.018*\"israeli\" + 0.008*\"nasa\" + 0.006*\"bike\" + 0.006*\"research\" + 0.006*\"center\" + 0.005*\"earth\" + 0.005*\"moon\" + 0.005*\"year\" + 0.005*\"ground\"'),\n",
       " (18,\n",
       "  '0.043*\"key\" + 0.026*\"encryption\" + 0.024*\"chip\" + 0.023*\"clipper\" + 0.022*\"netcom\" + 0.022*\"netcom com\" + 0.018*\"keys\" + 0.012*\"government\" + 0.011*\"clipper chip\" + 0.011*\"escrow\"'),\n",
       " (19,\n",
       "  '0.014*\"drive\" + 0.011*\"card\" + 0.009*\"scsi\" + 0.008*\"use\" + 0.008*\"dos\" + 0.008*\"disk\" + 0.007*\"mac\" + 0.007*\"thanks\" + 0.006*\"hard\" + 0.006*\"video\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vect = vect.transform(news_test)\n",
    "corpus = gensim.matutils.Sparse2Corpus(X_vect, documents_columns = False)\n",
    "topic_tuple_list = list(ldamodel[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 207,\n",
       " 1: 92,\n",
       " 2: 106,\n",
       " 3: 113,\n",
       " 4: 56,\n",
       " 5: 467,\n",
       " 6: 46,\n",
       " 7: 1762,\n",
       " 8: 693,\n",
       " 9: 556,\n",
       " 10: 74,\n",
       " 11: 80,\n",
       " 12: 209,\n",
       " 13: 111,\n",
       " 14: 285,\n",
       " 15: 676,\n",
       " 16: 372,\n",
       " 17: 277,\n",
       " 18: 153,\n",
       " 19: 1193}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(topic_tuple_list)\n",
    "y_predict = list()\n",
    "for topic_tuple in topic_tuple_list:\n",
    "    y_predict.append(sorted(topic_tuple, key = lambda x: x[1], reverse = True)[0][0])\n",
    "y_predict = np.array(y_predict)\n",
    "unique, counts = np.unique(y_predict, return_counts = True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 636 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lsamodel = gensim.models.LsiModel(corpus, num_topics = 6, id2word = id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.685*\"dos\" + 0.279*\"windows\" + 0.205*\"jpeg\" + 0.140*\"image\" + 0.135*\"microsoft\" + 0.134*\"software\" + 0.133*\"graphics\" + 0.131*\"microsoft windows\" + 0.124*\"file\" + 0.117*\"version\"'),\n",
       " (1,\n",
       "  '0.478*\"jpeg\" + -0.451*\"dos\" + 0.293*\"image\" + 0.215*\"file\" + 0.196*\"gif\" + 0.146*\"images\" + 0.132*\"format\" + -0.130*\"windows\" + 0.117*\"files\" + 0.112*\"color\"'),\n",
       " (2,\n",
       "  '-0.387*\"god\" + -0.362*\"jehovah\" + -0.289*\"lord\" + 0.248*\"jpeg\" + -0.178*\"christ\" + -0.177*\"said\" + -0.139*\"father\" + -0.133*\"jesus\" + -0.103*\"say\" + -0.102*\"did\"'),\n",
       " (3,\n",
       "  '-0.406*\"jpeg\" + -0.304*\"jehovah\" + -0.241*\"god\" + -0.231*\"lord\" + 0.153*\"data\" + -0.153*\"gif\" + 0.150*\"ftp\" + 0.149*\"graphics\" + -0.129*\"christ\" + 0.128*\"pub\"'),\n",
       " (4,\n",
       "  '0.244*\"jehovah\" + 0.222*\"graphics\" + -0.203*\"president\" + 0.173*\"lord\" + 0.169*\"pub\" + -0.166*\"jpeg\" + 0.157*\"ftp\" + -0.143*\"said\" + 0.130*\"data\" + -0.130*\"myers\"'),\n",
       " (5,\n",
       "  '-0.279*\"planet\" + -0.272*\"earth\" + 0.225*\"openwindows\" + -0.215*\"spacecraft\" + -0.210*\"solar\" + -0.207*\"venus\" + -0.175*\"surface\" + 0.140*\"use\" + -0.137*\"space\" + -0.137*\"atmosphere\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsamodel.print_topics(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vect = vect.transform(news_test)\n",
    "corpus = gensim.matutils.Sparse2Corpus(X_vect, documents_columns = False)\n",
    "topic_tuple_list = list(lsamodel[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-946377dfe3bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtopic_tuple\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopic_tuple_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0my_predict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_tuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# y_predict = np.array(y_predict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "y_predict = list()\n",
    "for topic_tuple in topic_tuple_list:\n",
    "    y_predict.append(sorted(topic_tuple, key = lambda x: x[1], reverse = True)[0])\n",
    "y_predict\n",
    "# y_predict = np.array(y_predict)\n",
    "# unique, counts = np.unique(y_predict, return_counts = True)\n",
    "# dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
