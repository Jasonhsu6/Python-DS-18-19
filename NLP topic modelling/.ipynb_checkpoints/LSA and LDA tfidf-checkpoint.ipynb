{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA and LDA to news group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training news: 11293\n",
      "number of test news: 7528\n"
     ]
    }
   ],
   "source": [
    "with open('20ng-train-all-terms.txt') as f:\n",
    "    train = f.read()\n",
    "with open('20ng-test-all-terms.txt') as f:\n",
    "    test = f.read()\n",
    "\n",
    "train = train[:-1]\n",
    "test = test[:-1]\n",
    "topic_train = []\n",
    "topic_test = []\n",
    "news_train = []\n",
    "news_test = []\n",
    "for new in train.split('\\n'):\n",
    "    topic_train.append(new.split('\\t')[0])\n",
    "    news_train.append(new.split('\\t')[1])\n",
    "print('number of training news: ' + str(len(news_train)))\n",
    "for new in test.split('\\n'):\n",
    "    topic_test.append(new.split('\\t')[0])\n",
    "    news_test.append(new.split('\\t')[1])\n",
    "print('number of test news: ' + str(len(news_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(topic_test)\n",
    "X_test = np.array(news_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of news: 11293\n",
      "number of topics: 7528\n"
     ]
    }
   ],
   "source": [
    "print('number of news: ' + str(len(news_train)))\n",
    "print('number of topics: ' + str(len(topic_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CountVectorizer to find three letter tokens, remove stop_words,\n",
    "# remove tokens that don't appear in at least 20 documents,\n",
    "# remove tokens that appear in more than 20% of the documents\n",
    "vect = CountVectorizer(min_df = 20, max_df = 0.2, stop_words = 'english', token_pattern = '(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
    "X = vect.fit_transform(news_train)\n",
    "\n",
    "# Convert sparse matrix to gensim corpus\n",
    "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns = False)\n",
    "id_map = dict((v, k) for k, v in vect.vocabulary_.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 6, id2word = id_map, passes = 25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"year\" + 0.007*\"car\" + 0.007*\"team\" + 0.006*\"game\" + 0.005*\"new\" + 0.004*\"play\" + 0.004*\"games\" + 0.004*\"better\" + 0.004*\"got\" + 0.004*\"hockey\"'),\n",
       " (1,\n",
       "  '0.009*\"use\" + 0.008*\"drive\" + 0.008*\"windows\" + 0.007*\"thanks\" + 0.007*\"problem\" + 0.006*\"need\" + 0.006*\"card\" + 0.005*\"work\" + 0.005*\"bit\" + 0.005*\"used\"'),\n",
       " (2,\n",
       "  '0.019*\"turkish\" + 0.017*\"armenian\" + 0.015*\"armenians\" + 0.010*\"armenia\" + 0.009*\"turkey\" + 0.009*\"said\" + 0.009*\"turks\" + 0.007*\"went\" + 0.006*\"soviet\" + 0.006*\"greek\"'),\n",
       " (3,\n",
       "  '0.007*\"government\" + 0.006*\"use\" + 0.006*\"key\" + 0.004*\"make\" + 0.004*\"law\" + 0.004*\"public\" + 0.004*\"used\" + 0.003*\"encryption\" + 0.003*\"say\" + 0.003*\"right\"'),\n",
       " (4,\n",
       "  '0.012*\"space\" + 0.010*\"file\" + 0.009*\"program\" + 0.007*\"information\" + 0.007*\"nasa\" + 0.006*\"available\" + 0.005*\"mail\" + 0.005*\"data\" + 0.005*\"use\" + 0.005*\"list\"'),\n",
       " (5,\n",
       "  '0.008*\"god\" + 0.005*\"did\" + 0.004*\"said\" + 0.004*\"say\" + 0.004*\"israel\" + 0.004*\"believe\" + 0.004*\"jesus\" + 0.004*\"way\" + 0.004*\"jews\" + 0.003*\"right\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vect = vect.transform(news_test)\n",
    "corpus = gensim.matutils.Sparse2Corpus(X_vect, documents_columns = False)\n",
    "topic_tuple_list = list(ldamodel[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1485, 1: 2067, 2: 71, 3: 902, 4: 1017, 5: 1986}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(topic_tuple_list)\n",
    "y_predict = list()\n",
    "for topic_tuple in topic_tuple_list:\n",
    "    y_predict.append(sorted(topic_tuple, key = lambda x: x[1], reverse = True)[0][0])\n",
    "y_predict = np.array(y_predict)\n",
    "unique, counts = np.unique(y_predict, return_counts = True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 550 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lsamodel = gensim.models.LsiModel(corpus, num_topics = 6, id2word = id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.681*\"dos\" + 0.279*\"windows\" + 0.221*\"jpeg\" + 0.150*\"image\" + 0.136*\"software\" + 0.136*\"graphics\" + 0.135*\"microsoft\" + 0.132*\"file\" + 0.120*\"version\" + 0.108*\"color\"'),\n",
       " (1,\n",
       "  '-0.481*\"dos\" + 0.476*\"jpeg\" + 0.290*\"image\" + 0.212*\"file\" + 0.195*\"gif\" + 0.145*\"images\" + -0.141*\"windows\" + 0.131*\"format\" + 0.114*\"files\" + 0.110*\"color\"'),\n",
       " (2,\n",
       "  '0.391*\"god\" + 0.367*\"jehovah\" + 0.293*\"lord\" + -0.242*\"jpeg\" + 0.180*\"christ\" + 0.178*\"said\" + 0.141*\"father\" + 0.134*\"jesus\" + 0.103*\"say\" + 0.102*\"did\"'),\n",
       " (3,\n",
       "  '0.405*\"jpeg\" + 0.304*\"jehovah\" + 0.238*\"god\" + 0.230*\"lord\" + -0.154*\"data\" + 0.152*\"gif\" + -0.149*\"ftp\" + -0.146*\"graphics\" + 0.133*\"dos\" + 0.128*\"christ\"'),\n",
       " (4,\n",
       "  '-0.240*\"jehovah\" + -0.228*\"graphics\" + 0.201*\"president\" + -0.174*\"pub\" + 0.174*\"jpeg\" + -0.170*\"lord\" + -0.163*\"ftp\" + 0.146*\"said\" + -0.136*\"data\" + 0.130*\"going\"'),\n",
       " (5,\n",
       "  '0.291*\"planet\" + 0.284*\"earth\" + 0.225*\"spacecraft\" + 0.219*\"solar\" + 0.215*\"venus\" + -0.205*\"openwindows\" + 0.182*\"surface\" + 0.143*\"atmosphere\" + 0.142*\"space\" + 0.138*\"moon\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsamodel.print_topics(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vect = vect.transform(news_test)\n",
    "corpus = gensim.matutils.Sparse2Corpus(X_vect, documents_columns = False)\n",
    "topic_tuple_list = list(lsamodel[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-946377dfe3bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtopic_tuple\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopic_tuple_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0my_predict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_tuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# y_predict = np.array(y_predict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "y_predict = list()\n",
    "for topic_tuple in topic_tuple_list:\n",
    "    y_predict.append(sorted(topic_tuple, key = lambda x: x[1], reverse = True)[0])\n",
    "y_predict\n",
    "# y_predict = np.array(y_predict)\n",
    "# unique, counts = np.unique(y_predict, return_counts = True)\n",
    "# dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
