{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA and LDA to news group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training news: 11293\n",
      "number of test news: 7528\n"
     ]
    }
   ],
   "source": [
    "with open('20ng-train-all-terms.txt') as f:\n",
    "    train = f.read()\n",
    "with open('20ng-test-all-terms.txt') as f:\n",
    "    test = f.read()\n",
    "\n",
    "train = train[:-1]\n",
    "test = test[:-1]\n",
    "topic_train = []\n",
    "topic_test = []\n",
    "news_train = []\n",
    "news_test = []\n",
    "for new in train.split('\\n'):\n",
    "    topic_train.append(new.split('\\t')[0])\n",
    "    news_train.append(new.split('\\t')[1])\n",
    "print('number of training news: ' + str(len(news_train)))\n",
    "for new in test.split('\\n'):\n",
    "    topic_test.append(new.split('\\t')[0])\n",
    "    news_test.append(new.split('\\t')[1])\n",
    "print('number of test news: ' + str(len(news_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(topic_test)\n",
    "X_test = np.array(news_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of news: 11293\n",
      "number of topics: 7528\n"
     ]
    }
   ],
   "source": [
    "print('number of news: ' + str(len(news_train)))\n",
    "print('number of topics: ' + str(len(topic_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CountVectorizer to find three letter tokens, remove stop_words,\n",
    "# remove tokens that don't appear in at least 20 documents,\n",
    "# remove tokens that appear in more than 20% of the documents\n",
    "# vect = CountVectorizer(min_df = 20, max_df = 0.2, stop_words = 'english', token_pattern = '(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b', \n",
    "#                        ngram_range = (1, 2))\n",
    "vect = CountVectorizer(min_df = 20, max_df = 0.2, stop_words = 'english', token_pattern = '(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
    "X = vect.fit_transform(news_train)\n",
    "\n",
    "# Convert sparse matrix to gensim corpus\n",
    "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns = False)\n",
    "id_map = dict((v, k) for k, v in vect.vocabulary_.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 20, id2word = id_map, passes = 25, random_state = 0)\n",
    "# ldamodel = gensim.models.LdaMulticore(corpus, num_topics = 6, id2word = id_map, passes = 25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.028*\"apple\" + 0.027*\"sandvik\" + 0.024*\"kent\" + 0.014*\"newton\" + 0.013*\"engine\" + 0.013*\"gary\" + 0.013*\"saturn\" + 0.011*\"tank\" + 0.011*\"gld\" + 0.010*\"picture\"'),\n",
       " (1,\n",
       "  '0.029*\"drive\" + 0.023*\"card\" + 0.018*\"scsi\" + 0.014*\"mac\" + 0.012*\"disk\" + 0.012*\"video\" + 0.012*\"bit\" + 0.011*\"speed\" + 0.011*\"drivers\" + 0.010*\"hard\"'),\n",
       " (2,\n",
       "  '0.099*\"israel\" + 0.033*\"arab\" + 0.032*\"jewish\" + 0.026*\"land\" + 0.019*\"arabs\" + 0.016*\"lebanese\" + 0.016*\"lebanon\" + 0.014*\"israelis\" + 0.013*\"palestine\" + 0.013*\"center\"'),\n",
       " (3,\n",
       "  '0.019*\"russian\" + 0.019*\"hockey\" + 0.017*\"season\" + 0.014*\"players\" + 0.013*\"team\" + 0.012*\"league\" + 0.011*\"nhl\" + 0.011*\"los\" + 0.011*\"division\" + 0.010*\"angeles\"'),\n",
       " (4,\n",
       "  '0.018*\"information\" + 0.010*\"public\" + 0.009*\"security\" + 0.008*\"use\" + 0.008*\"mail\" + 0.008*\"list\" + 0.008*\"new\" + 0.008*\"data\" + 0.008*\"technology\" + 0.007*\"available\"'),\n",
       " (5,\n",
       "  '0.009*\"uiuc\" + 0.008*\"news\" + 0.007*\"really\" + 0.006*\"cso\" + 0.006*\"baseball\" + 0.006*\"mike\" + 0.006*\"day\" + 0.006*\"frank\" + 0.005*\"secretary\" + 0.005*\"bob\"'),\n",
       " (6,\n",
       "  '0.015*\"said\" + 0.012*\"gun\" + 0.009*\"did\" + 0.008*\"didn\" + 0.007*\"right\" + 0.006*\"went\" + 0.006*\"killed\" + 0.006*\"says\" + 0.006*\"say\" + 0.006*\"guns\"'),\n",
       " (7,\n",
       "  '0.017*\"car\" + 0.010*\"new\" + 0.009*\"water\" + 0.009*\"left\" + 0.007*\"gay\" + 0.007*\"cars\" + 0.007*\"nuclear\" + 0.006*\"dod\" + 0.005*\"door\" + 0.005*\"doctor\"'),\n",
       " (8,\n",
       "  '0.021*\"jews\" + 0.020*\"turkish\" + 0.018*\"armenian\" + 0.017*\"armenians\" + 0.010*\"armenia\" + 0.010*\"human\" + 0.009*\"men\" + 0.008*\"women\" + 0.008*\"children\" + 0.007*\"rights\"'),\n",
       " (9,\n",
       "  '0.075*\"key\" + 0.039*\"chip\" + 0.039*\"encryption\" + 0.036*\"clipper\" + 0.024*\"netcom\" + 0.018*\"secret\" + 0.016*\"escrow\" + 0.015*\"government\" + 0.015*\"law\" + 0.014*\"algorithm\"'),\n",
       " (10,\n",
       "  '0.011*\"new\" + 0.010*\"turkey\" + 0.008*\"price\" + 0.007*\"sale\" + 0.007*\"used\" + 0.007*\"power\" + 0.006*\"bike\" + 0.005*\"ground\" + 0.005*\"air\" + 0.005*\"box\"'),\n",
       " (11,\n",
       "  '0.045*\"israeli\" + 0.012*\"state\" + 0.011*\"ohio\" + 0.010*\"university\" + 0.009*\"acs\" + 0.009*\"ibm\" + 0.009*\"virginia\" + 0.008*\"org\" + 0.008*\"internet\" + 0.008*\"fax\"'),\n",
       " (12,\n",
       "  '0.026*\"windows\" + 0.023*\"thanks\" + 0.018*\"help\" + 0.014*\"mail\" + 0.014*\"dos\" + 0.013*\"problem\" + 0.012*\"use\" + 0.012*\"need\" + 0.008*\"advance\" + 0.007*\"email\"'),\n",
       " (13,\n",
       "  '0.020*\"government\" + 0.019*\"president\" + 0.011*\"clinton\" + 0.011*\"states\" + 0.010*\"money\" + 0.009*\"state\" + 0.008*\"american\" + 0.008*\"year\" + 0.008*\"turks\" + 0.008*\"tax\"'),\n",
       " (14,\n",
       "  '0.048*\"space\" + 0.027*\"nasa\" + 0.020*\"gov\" + 0.011*\"earth\" + 0.011*\"launch\" + 0.011*\"moon\" + 0.008*\"shuttle\" + 0.007*\"lunar\" + 0.007*\"henry\" + 0.007*\"jpl\"'),\n",
       " (15,\n",
       "  '0.010*\"make\" + 0.008*\"way\" + 0.007*\"want\" + 0.007*\"use\" + 0.006*\"better\" + 0.006*\"point\" + 0.006*\"say\" + 0.006*\"right\" + 0.006*\"really\" + 0.005*\"going\"'),\n",
       " (16,\n",
       "  '0.017*\"file\" + 0.010*\"program\" + 0.008*\"code\" + 0.008*\"available\" + 0.007*\"use\" + 0.007*\"software\" + 0.007*\"image\" + 0.006*\"version\" + 0.006*\"data\" + 0.006*\"window\"'),\n",
       " (17,\n",
       "  '0.018*\"york\" + 0.016*\"nazi\" + 0.015*\"san\" + 0.014*\"new\" + 0.014*\"gas\" + 0.011*\"nazis\" + 0.011*\"boston\" + 0.010*\"german\" + 0.009*\"holocaust\" + 0.009*\"columbia\"'),\n",
       " (18,\n",
       "  '0.033*\"game\" + 0.025*\"team\" + 0.024*\"games\" + 0.024*\"play\" + 0.020*\"year\" + 0.020*\"win\" + 0.013*\"player\" + 0.013*\"msg\" + 0.011*\"pittsburgh\" + 0.011*\"myers\"'),\n",
       " (19,\n",
       "  '0.026*\"god\" + 0.012*\"jesus\" + 0.009*\"christian\" + 0.009*\"believe\" + 0.007*\"bible\" + 0.007*\"say\" + 0.007*\"christians\" + 0.006*\"church\" + 0.006*\"life\" + 0.006*\"christ\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vect = vect.transform(news_test)\n",
    "corpus = gensim.matutils.Sparse2Corpus(X_vect, documents_columns = False)\n",
    "topic_tuple_list = list(ldamodel[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 35,\n",
       " 1: 508,\n",
       " 2: 32,\n",
       " 3: 111,\n",
       " 4: 265,\n",
       " 5: 559,\n",
       " 6: 440,\n",
       " 7: 256,\n",
       " 8: 171,\n",
       " 9: 58,\n",
       " 10: 742,\n",
       " 11: 132,\n",
       " 12: 879,\n",
       " 13: 97,\n",
       " 14: 114,\n",
       " 15: 1499,\n",
       " 16: 577,\n",
       " 17: 30,\n",
       " 18: 197,\n",
       " 19: 826}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(topic_tuple_list)\n",
    "y_predict = list()\n",
    "for topic_tuple in topic_tuple_list:\n",
    "    y_predict.append(sorted(topic_tuple, key = lambda x: x[1], reverse = True)[0][0])\n",
    "y_predict = np.array(y_predict)\n",
    "unique, counts = np.unique(y_predict, return_counts = True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 601 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lsamodel = gensim.models.LsiModel(corpus, num_topics = 20, id2word = id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '-0.681*\"dos\" + -0.279*\"windows\" + -0.221*\"jpeg\" + -0.150*\"image\" + -0.136*\"software\" + -0.136*\"graphics\" + -0.135*\"microsoft\" + -0.132*\"file\" + -0.120*\"version\" + -0.108*\"color\"'),\n",
       " (1,\n",
       "  '0.481*\"dos\" + -0.476*\"jpeg\" + -0.290*\"image\" + -0.212*\"file\" + -0.195*\"gif\" + -0.145*\"images\" + 0.141*\"windows\" + -0.131*\"format\" + -0.114*\"files\" + -0.110*\"color\"'),\n",
       " (2,\n",
       "  '-0.391*\"god\" + -0.367*\"jehovah\" + -0.293*\"lord\" + 0.242*\"jpeg\" + -0.180*\"christ\" + -0.178*\"said\" + -0.141*\"father\" + -0.134*\"jesus\" + -0.103*\"say\" + -0.102*\"did\"'),\n",
       " (3,\n",
       "  '-0.405*\"jpeg\" + -0.304*\"jehovah\" + -0.238*\"god\" + -0.230*\"lord\" + 0.154*\"data\" + -0.152*\"gif\" + 0.149*\"ftp\" + 0.146*\"graphics\" + -0.133*\"dos\" + -0.128*\"christ\"'),\n",
       " (4,\n",
       "  '0.240*\"jehovah\" + 0.228*\"graphics\" + -0.201*\"president\" + 0.174*\"pub\" + -0.174*\"jpeg\" + 0.170*\"lord\" + 0.163*\"ftp\" + -0.146*\"said\" + 0.136*\"data\" + -0.130*\"going\"'),\n",
       " (5,\n",
       "  '-0.291*\"planet\" + -0.284*\"earth\" + -0.225*\"spacecraft\" + -0.219*\"solar\" + -0.215*\"venus\" + 0.205*\"openwindows\" + -0.182*\"surface\" + -0.143*\"atmosphere\" + -0.142*\"space\" + -0.138*\"moon\"'),\n",
       " (6,\n",
       "  '0.335*\"openwindows\" + 0.215*\"sun\" + -0.206*\"graphics\" + 0.191*\"use\" + 0.172*\"xview\" + -0.158*\"president\" + 0.153*\"planet\" + 0.151*\"usr\" + 0.148*\"earth\" + -0.147*\"image\"'),\n",
       " (7,\n",
       "  '0.303*\"myers\" + 0.291*\"president\" + -0.231*\"cancer\" + -0.165*\"god\" + -0.156*\"health\" + -0.147*\"medical\" + -0.136*\"drug\" + -0.122*\"disease\" + 0.120*\"said\" + -0.118*\"patients\"'),\n",
       " (8,\n",
       "  '0.330*\"mac\" + -0.329*\"image\" + 0.208*\"files\" + 0.200*\"disk\" + 0.188*\"comp\" + 0.157*\"sys\" + -0.150*\"openwindows\" + 0.149*\"god\" + 0.143*\"faq\" + 0.143*\"file\"'),\n",
       " (9,\n",
       "  '-0.311*\"god\" + 0.265*\"jehovah\" + 0.216*\"cancer\" + 0.197*\"myers\" + 0.174*\"president\" + 0.147*\"health\" + 0.143*\"mac\" + 0.127*\"medical\" + 0.125*\"lord\" + 0.117*\"drug\"'),\n",
       " (10,\n",
       "  '0.301*\"image\" + 0.235*\"god\" + -0.191*\"armenians\" + 0.182*\"myers\" + -0.181*\"armenian\" + -0.162*\"graphics\" + -0.160*\"said\" + 0.150*\"president\" + 0.132*\"data\" + -0.129*\"jehovah\"'),\n",
       " (11,\n",
       "  '0.324*\"image\" + -0.232*\"graphics\" + -0.195*\"god\" + 0.185*\"mac\" + -0.157*\"jpeg\" + 0.156*\"data\" + -0.150*\"pub\" + 0.143*\"software\" + -0.137*\"myers\" + 0.135*\"armenians\"'),\n",
       " (12,\n",
       "  '0.226*\"azerbaijan\" + 0.184*\"armenian\" + -0.178*\"said\" + 0.176*\"russian\" + 0.138*\"turkish\" + -0.138*\"cancer\" + -0.134*\"started\" + -0.133*\"door\" + -0.131*\"didn\" + -0.128*\"children\"'),\n",
       " (13,\n",
       "  '-0.362*\"god\" + -0.241*\"myers\" + -0.206*\"armenian\" + -0.190*\"azerbaijan\" + 0.173*\"jehovah\" + -0.157*\"armenians\" + -0.135*\"russian\" + 0.133*\"right\" + 0.120*\"israel\" + -0.113*\"cancer\"'),\n",
       " (14,\n",
       "  '0.478*\"pit\" + 0.405*\"det\" + 0.320*\"bos\" + 0.310*\"tor\" + 0.282*\"chi\" + 0.178*\"van\" + 0.175*\"que\" + 0.172*\"buf\" + 0.165*\"stl\" + 0.165*\"nyi\"'),\n",
       " (15,\n",
       "  '-0.190*\"israel\" + -0.162*\"fbi\" + 0.160*\"use\" + -0.160*\"information\" + 0.147*\"azerbaijan\" + -0.147*\"san\" + -0.144*\"police\" + -0.143*\"francisco\" + -0.134*\"files\" + -0.132*\"anti\"'),\n",
       " (16,\n",
       "  '-0.344*\"homosexuality\" + 0.280*\"god\" + -0.274*\"homosexual\" + -0.262*\"paul\" + -0.147*\"christians\" + -0.141*\"jehovah\" + -0.132*\"sex\" + -0.126*\"ftp\" + 0.116*\"slip\" + 0.108*\"file\"'),\n",
       " (17,\n",
       "  '0.492*\"myers\" + 0.182*\"homosexuality\" + -0.181*\"did\" + -0.174*\"president\" + 0.152*\"homosexual\" + 0.146*\"paul\" + -0.138*\"general\" + -0.138*\"ftp\" + -0.128*\"free\" + 0.109*\"image\"'),\n",
       " (18,\n",
       "  '0.345*\"myers\" + -0.277*\"image\" + 0.207*\"data\" + 0.161*\"ftp\" + 0.141*\"free\" + -0.139*\"bit\" + -0.126*\"president\" + 0.123*\"available\" + 0.119*\"jpeg\" + -0.116*\"graphics\"'),\n",
       " (19,\n",
       "  '0.259*\"slip\" + 0.181*\"file\" + 0.167*\"phone\" + 0.159*\"use\" + -0.150*\"myers\" + 0.148*\"driver\" + 0.148*\"data\" + 0.146*\"homosexuality\" + -0.140*\"image\" + 0.122*\"packet\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsamodel.print_topics(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vect = vect.transform(news_test)\n",
    "corpus = gensim.matutils.Sparse2Corpus(X_vect, documents_columns = False)\n",
    "topic_tuple_list = list(lsamodel[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-946377dfe3bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtopic_tuple\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopic_tuple_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0my_predict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_tuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# y_predict = np.array(y_predict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "y_predict = list()\n",
    "for topic_tuple in topic_tuple_list:\n",
    "    y_predict.append(sorted(topic_tuple, key = lambda x: x[1], reverse = True)[0])\n",
    "y_predict\n",
    "# y_predict = np.array(y_predict)\n",
    "# unique, counts = np.unique(y_predict, return_counts = True)\n",
    "# dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
